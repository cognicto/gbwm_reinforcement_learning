[
  {
    "iteration": 1,
    "total_timesteps": 76800,
    "total_episodes": 4800,
    "mean_episode_reward": 26.6,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.09969308152329176,
    "value_loss": 157.52716345469156,
    "mean_advantage": 0.0,
    "mean_return": 14.755900382995605
  },
  {
    "iteration": 2,
    "total_timesteps": 153600,
    "total_episodes": 9600,
    "mean_episode_reward": 38.4,
    "mean_episode_length": 16.0,
    "policy_lr": 0.008,
    "value_lr": 0.008,
    "policy_loss": -0.07757549994780372,
    "value_loss": 122.83536483764648,
    "mean_advantage": -4.609425729995564e-08,
    "mean_return": 22.698413848876953
  },
  {
    "iteration": 3,
    "total_timesteps": 230400,
    "total_episodes": 14400,
    "mean_episode_reward": 48.2,
    "mean_episode_length": 16.0,
    "policy_lr": 0.007,
    "value_lr": 0.007,
    "policy_loss": -0.05411627439471583,
    "value_loss": 70.70727661132813,
    "mean_advantage": 2.8610228852699038e-08,
    "mean_return": 27.55704689025879
  },
  {
    "iteration": 4,
    "total_timesteps": 307200,
    "total_episodes": 19200,
    "mean_episode_reward": 52.28,
    "mean_episode_length": 16.0,
    "policy_lr": 0.006000000000000001,
    "value_lr": 0.006000000000000001,
    "policy_loss": -0.03865912604611367,
    "value_loss": 25.148150192896527,
    "mean_advantage": 1.5894572324981482e-09,
    "mean_return": 30.104076385498047
  },
  {
    "iteration": 5,
    "total_timesteps": 384000,
    "total_episodes": 24000,
    "mean_episode_reward": 53.86,
    "mean_episode_length": 16.0,
    "policy_lr": 0.005000000000000001,
    "value_lr": 0.005000000000000001,
    "policy_loss": -0.03213880082747589,
    "value_loss": 7.467390069762866,
    "mean_advantage": -7.947286384535346e-09,
    "mean_return": 30.739967346191406
  },
  {
    "iteration": 6,
    "total_timesteps": 460800,
    "total_episodes": 28800,
    "mean_episode_reward": 53.78,
    "mean_episode_length": 16.0,
    "policy_lr": 0.004000000000000001,
    "value_lr": 0.004000000000000001,
    "policy_loss": -0.01889749365237852,
    "value_loss": 3.0696542662382127,
    "mean_advantage": -2.543131571997037e-08,
    "mean_return": 30.779590606689453
  },
  {
    "iteration": 7,
    "total_timesteps": 537600,
    "total_episodes": 33600,
    "mean_episode_reward": 47.7,
    "mean_episode_length": 16.0,
    "policy_lr": 0.003000000000000001,
    "value_lr": 0.003000000000000001,
    "policy_loss": -0.06840324351719271,
    "value_loss": 14.830205346743266,
    "mean_advantage": 1.3113021779531664e-08,
    "mean_return": 29.269817352294922
  },
  {
    "iteration": 8,
    "total_timesteps": 614400,
    "total_episodes": 38400,
    "mean_episode_reward": 51.62,
    "mean_episode_length": 16.0,
    "policy_lr": 0.002000000000000001,
    "value_lr": 0.002000000000000001,
    "policy_loss": -0.019802416973592092,
    "value_loss": 9.012312870820363,
    "mean_advantage": 1.7484028447256605e-08,
    "mean_return": 30.130475997924805
  },
  {
    "iteration": 9,
    "total_timesteps": 691200,
    "total_episodes": 43200,
    "mean_episode_reward": 48.4,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0010000000000000005,
    "value_lr": 0.0010000000000000005,
    "policy_loss": -0.05543213619928186,
    "value_loss": 42.448348172505696,
    "mean_advantage": -1.390775050680304e-09,
    "mean_return": 28.360456466674805
  },
  {
    "iteration": 10,
    "total_timesteps": 768000,
    "total_episodes": 48000,
    "mean_episode_reward": 52.06,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0,
    "value_lr": 0.0,
    "policy_loss": -0.04500495441878835,
    "value_loss": 24.963714129924774,
    "mean_advantage": -5.761782162494455e-09,
    "mean_return": 29.487388610839844
  }
]