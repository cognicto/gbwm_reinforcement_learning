{
  "iteration": 0,
  "metrics": {
    "iteration": 1,
    "total_timesteps": 800,
    "total_episodes": 50,
    "mean_episode_reward": 10.44,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.04939293535426259,
    "value_loss": 87.5768210887909,
    "mean_advantage": 3.33786012163273e-08,
    "mean_return": 5.642999172210693
  },
  "config": {
    "n_traj": 500,
    "batch_size": 50,
    "learning_rate": 0.01,
    "clip_epsilon": 0.5,
    "n_neurons": 64,
    "ppo_epochs": 4,
    "mini_batch_size": 256,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "entropy_coeff": 0.01,
    "value_loss_coeff": 0.5,
    "max_grad_norm": 0.5,
    "time_horizon": 16,
    "num_goals": 2,
    "num_portfolios": 15,
    "initial_wealth_base": 12.0,
    "wealth_scaling": 0.85,
    "device": "cpu",
    "random_seed": 42,
    "log_interval": 10,
    "save_interval": 50,
    "eval_interval": 20,
    "data_mode": "simulation",
    "historical_data_path": "data/raw/market_data/",
    "historical_validation_episodes": 1000,
    "historical_augmentation": true,
    "initial_wealth": 216300.11102659925
  },
  "training_history": [
    {
      "iteration": 1,
      "total_timesteps": 800,
      "total_episodes": 50,
      "mean_episode_reward": 10.44,
      "mean_episode_length": 16.0,
      "policy_lr": 0.009000000000000001,
      "value_lr": 0.009000000000000001,
      "policy_loss": -0.04939293535426259,
      "value_loss": 87.5768210887909,
      "mean_advantage": 3.33786012163273e-08,
      "mean_return": 5.642999172210693
    }
  ]
}