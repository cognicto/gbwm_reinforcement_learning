[
  {
    "iteration": 1,
    "total_timesteps": 76800,
    "total_episodes": 4800,
    "mean_episode_reward": 26.38,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.0991099941668411,
    "value_loss": 154.3453795560201,
    "mean_advantage": 1.0172526287988148e-07,
    "mean_return": 15.356175422668457
  },
  {
    "iteration": 2,
    "total_timesteps": 153600,
    "total_episodes": 9600,
    "mean_episode_reward": 43.58,
    "mean_episode_length": 16.0,
    "policy_lr": 0.008,
    "value_lr": 0.008,
    "policy_loss": -0.07328008667565883,
    "value_loss": 119.51675118764241,
    "mean_advantage": -1.3510386587256562e-08,
    "mean_return": 23.435962677001953
  },
  {
    "iteration": 3,
    "total_timesteps": 230400,
    "total_episodes": 14400,
    "mean_episode_reward": 45.5,
    "mean_episode_length": 16.0,
    "policy_lr": 0.007,
    "value_lr": 0.007,
    "policy_loss": -0.05404486413657044,
    "value_loss": 73.59818683624268,
    "mean_advantage": -3.258387337723434e-08,
    "mean_return": 27.34718132019043
  },
  {
    "iteration": 4,
    "total_timesteps": 307200,
    "total_episodes": 19200,
    "mean_episode_reward": 51.48,
    "mean_episode_length": 16.0,
    "policy_lr": 0.006000000000000001,
    "value_lr": 0.006000000000000001,
    "policy_loss": -0.041494547005277126,
    "value_loss": 38.07637468655904,
    "mean_advantage": -3.1789144649962964e-09,
    "mean_return": 29.539011001586914
  },
  {
    "iteration": 5,
    "total_timesteps": 384000,
    "total_episodes": 24000,
    "mean_episode_reward": 53.46,
    "mean_episode_length": 16.0,
    "policy_lr": 0.005000000000000001,
    "value_lr": 0.005000000000000001,
    "policy_loss": -0.035359261992853136,
    "value_loss": 21.965244157314302,
    "mean_advantage": 1.5894572324981482e-09,
    "mean_return": 30.34014320373535
  },
  {
    "iteration": 6,
    "total_timesteps": 460800,
    "total_episodes": 28800,
    "mean_episode_reward": 53.6,
    "mean_episode_length": 16.0,
    "policy_lr": 0.004000000000000001,
    "value_lr": 0.004000000000000001,
    "policy_loss": -0.03187730210755641,
    "value_loss": 10.232614036798477,
    "mean_advantage": -2.7020773174513124e-08,
    "mean_return": 30.686992645263672
  },
  {
    "iteration": 7,
    "total_timesteps": 537600,
    "total_episodes": 33600,
    "mean_episode_reward": 53.86,
    "mean_episode_length": 16.0,
    "policy_lr": 0.003000000000000001,
    "value_lr": 0.003000000000000001,
    "policy_loss": -0.030595372943983724,
    "value_loss": 3.022924412228167,
    "mean_advantage": 0.0,
    "mean_return": 30.776256561279297
  },
  {
    "iteration": 8,
    "total_timesteps": 614400,
    "total_episodes": 38400,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.002000000000000001,
    "value_lr": 0.002000000000000001,
    "policy_loss": -0.03153807514968018,
    "value_loss": 2.1323789595843605,
    "mean_advantage": 7.947286384535346e-09,
    "mean_return": 30.770917892456055
  },
  {
    "iteration": 9,
    "total_timesteps": 691200,
    "total_episodes": 43200,
    "mean_episode_reward": 53.78,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0010000000000000005,
    "value_lr": 0.0010000000000000005,
    "policy_loss": -0.031120229290487866,
    "value_loss": 2.1976475011464207,
    "mean_advantage": 3.973643192267673e-09,
    "mean_return": 30.75910186767578
  },
  {
    "iteration": 10,
    "total_timesteps": 768000,
    "total_episodes": 48000,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0,
    "value_lr": 0.0,
    "policy_loss": -0.030427271705896906,
    "value_loss": 0.936194859072178,
    "mean_advantage": -9.53674295089968e-09,
    "mean_return": 30.8155460357666
  }
]