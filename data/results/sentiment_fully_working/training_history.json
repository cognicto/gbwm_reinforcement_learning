[
  {
    "iteration": 1,
    "total_timesteps": 76800,
    "total_episodes": 4800,
    "mean_episode_reward": 27.66,
    "mean_episode_length": 16.0,
    "mean_goal_success_rate": 0.51,
    "policy_lr": 0.0091,
    "value_lr": 0.0091,
    "policy_loss": -0.09951119859547665,
    "value_loss": 150.419485912323,
    "mean_advantage": 6.596247459356164e-08,
    "mean_return": 15.524699211120605,
    "mean_entropy": 