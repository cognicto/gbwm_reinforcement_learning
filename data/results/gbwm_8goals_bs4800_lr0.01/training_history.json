[
  {
    "iteration": 1,
    "total_timesteps": 76800,
    "total_episodes": 4800,
    "mean_episode_reward": 50.14,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.13556850431486964,
    "value_loss": 378.39778998057045,
    "mean_advantage": 1.4305114426349519e-08,
    "mean_return": 30.90861701965332
  },
  {
    "iteration": 2,
    "total_timesteps": 153600,
    "total_episodes": 9600,
    "mean_episode_reward": 87.58,
    "mean_episode_length": 16.0,
    "policy_lr": 0.008,
    "value_lr": 0.008,
    "policy_loss": -0.10708671730943024,
    "value_loss": 325.56090075174967,
    "mean_advantage": 5.563100202721216e-08,
    "mean_return": 51.19671630859375
  },
  {
    "iteration": 3,
    "total_timesteps": 230400,
    "total_episodes": 14400,
    "mean_episode_reward": 109.58,
    "mean_episode_length": 16.0,
    "policy_lr": 0.007,
    "value_lr": 0.007,
    "policy_loss": -0.07291734653913105,
    "value_loss": 190.20994674682618,
    "mean_advantage": -6.993611378902642e-08,
    "mean_return": 63.4266357421875
  },
  {
    "iteration": 4,
    "total_timesteps": 307200,
    "total_episodes": 19200,
    "mean_episode_reward": 120.82,
    "mean_episode_length": 16.0,
    "policy_lr": 0.006000000000000001,
    "value_lr": 0.006000000000000001,
    "policy_loss": -0.05252113165644308,
    "value_loss": 84.23391046524048,
    "mean_advantage": 1.2715657859985185e-08,
    "mean_return": 70.39250183105469
  },
  {
    "iteration": 5,
    "total_timesteps": 384000,
    "total_episodes": 24000,
    "mean_episode_reward": 124.28,
    "mean_episode_length": 16.0,
    "policy_lr": 0.005000000000000001,
    "value_lr": 0.005000000000000001,
    "policy_loss": -0.03593437162227928,
    "value_loss": 39.18465272744497,
    "mean_advantage": 1.2715657859985185e-08,
    "mean_return": 72.94755554199219
  },
  {
    "iteration": 6,
    "total_timesteps": 460800,
    "total_episodes": 28800,
    "mean_episode_reward": 125.78,
    "mean_episode_length": 16.0,
    "policy_lr": 0.004000000000000001,
    "value_lr": 0.004000000000000001,
    "policy_loss": -0.029907504280563445,
    "value_loss": 24.586040658950807,
    "mean_advantage": 0.0,
    "mean_return": 73.67931365966797
  },
  {
    "iteration": 7,
    "total_timesteps": 537600,
    "total_episodes": 33600,
    "mean_episode_reward": 125.8,
    "mean_episode_length": 16.0,
    "policy_lr": 0.003000000000000001,
    "value_lr": 0.003000000000000001,
    "policy_loss": -0.028667365750297904,
    "value_loss": 19.763412160873415,
    "mean_advantage": 0.0,
    "mean_return": 73.89714813232422
  },
  {
    "iteration": 8,
    "total_timesteps": 614400,
    "total_episodes": 38400,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.002000000000000001,
    "value_lr": 0.002000000000000001,
    "policy_loss": -0.02895219004324948,
    "value_loss": 19.072828585306805,
    "mean_advantage": 2.543131571997037e-08,
    "mean_return": 73.9287338256836
  },
  {
    "iteration": 9,
    "total_timesteps": 691200,
    "total_episodes": 43200,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0010000000000000005,
    "value_lr": 0.0010000000000000005,
    "policy_loss": -0.027691899527950835,
    "value_loss": 18.46351859807968,
    "mean_advantage": 6.357828929992593e-09,
    "mean_return": 73.95307922363281
  },
  {
    "iteration": 10,
    "total_timesteps": 768000,
    "total_episodes": 48000,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0,
    "value_lr": 0.0,
    "policy_loss": -0.027383456747823706,
    "value_loss": 18.477981878916424,
    "mean_advantage": 1.907348590179936e-08,
    "mean_return": 73.9433822631836
  }
]