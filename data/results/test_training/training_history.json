[
  {
    "iteration": 1,
    "total_timesteps": 3200,
    "total_episodes": 200,
    "mean_episode_reward": 30.9,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.08061627599482353,
    "value_loss": 349.6725117610051,
    "mean_advantage": 2.1457672971791908e-08,
    "mean_return": 17.32156753540039
  },
  {
    "iteration": 2,
    "total_timesteps": 6400,
    "total_episodes": 400,
    "mean_episode_reward": 43.12,
    "mean_episode_length": 16.0,
    "policy_lr": 0.008,
    "value_lr": 0.008,
    "policy_loss": -0.05624897011484091,
    "value_loss": 149.11820397010217,
    "mean_advantage": -2.3841858265427618e-08,
    "mean_return": 24.188888549804688
  },
  {
    "iteration": 3,
    "total_timesteps": 9600,
    "total_episodes": 600,
    "mean_episode_reward": 47.12,
    "mean_episode_length": 16.0,
    "policy_lr": 0.007,
    "value_lr": 0.007,
    "policy_loss": -0.05548492546838064,
    "value_loss": 92.50674379788913,
    "mean_advantage": 3.814697180359872e-08,
    "mean_return": 27.085962295532227
  },
  {
    "iteration": 4,
    "total_timesteps": 12800,
    "total_episodes": 800,
    "mean_episode_reward": 51.98,
    "mean_episode_length": 16.0,
    "policy_lr": 0.006000000000000001,
    "value_lr": 0.006000000000000001,
    "policy_loss": -0.0474610452253658,
    "value_loss": 54.695701085604156,
    "mean_advantage": 1.907348590179936e-08,
    "mean_return": 29.46414566040039
  },
  {
    "iteration": 5,
    "total_timesteps": 16000,
    "total_episodes": 1000,
    "mean_episode_reward": 53.06,
    "mean_episode_length": 16.0,
    "policy_lr": 0.005000000000000001,
    "value_lr": 0.005000000000000001,
    "policy_loss": -0.03589060834537332,
    "value_loss": 33.82176091120793,
    "mean_advantage": -1.907348590179936e-08,
    "mean_return": 30.444311141967773
  },
  {
    "iteration": 6,
    "total_timesteps": 19200,
    "total_episodes": 1200,
    "mean_episode_reward": 53.86,
    "mean_episode_length": 16.0,
    "policy_lr": 0.004000000000000001,
    "value_lr": 0.004000000000000001,
    "policy_loss": -0.0408713361248374,
    "value_loss": 24.98164151265071,
    "mean_advantage": -2.8610228852699038e-08,
    "mean_return": 30.75289535522461
  },
  {
    "iteration": 7,
    "total_timesteps": 22400,
    "total_episodes": 1400,
    "mean_episode_reward": 53.5,
    "mean_episode_length": 16.0,
    "policy_lr": 0.003000000000000001,
    "value_lr": 0.003000000000000001,
    "policy_loss": -0.029174350237903688,
    "value_loss": 25.513233991769646,
    "mean_advantage": -1.907348590179936e-08,
    "mean_return": 30.655603408813477
  },
  {
    "iteration": 8,
    "total_timesteps": 25600,
    "total_episodes": 1600,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.002000000000000001,
    "value_lr": 0.002000000000000001,
    "policy_loss": -0.02712282040514625,
    "value_loss": 22.682418016286995,
    "mean_advantage": 9.53674295089968e-09,
    "mean_return": 30.815658569335938
  },
  {
    "iteration": 9,
    "total_timesteps": 28800,
    "total_episodes": 1800,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0010000000000000005,
    "value_lr": 0.0010000000000000005,
    "policy_loss": -0.034090120488634475,
    "value_loss": 22.072094917297363,
    "mean_advantage": 4.7683716530855236e-08,
    "mean_return": 30.858543395996094
  },
  {
    "iteration": 10,
    "total_timesteps": 32000,
    "total_episodes": 2000,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0,
    "value_lr": 0.0,
    "policy_loss": -0.030292845725153502,
    "value_loss": 21.971749232365536,
    "mean_advantage": -9.53674295089968e-09,
    "mean_return": 30.858543395996094
  }
]