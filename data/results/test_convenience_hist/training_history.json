[
  {
    "iteration": 1,
    "total_timesteps": 640,
    "total_episodes": 40,
    "mean_episode_reward": 11.25,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.06466334592550993,
    "value_loss": 97.60218238830566,
    "mean_advantage": 2.3841858265427618e-08,
    "mean_return": 6.080818176269531
  },
  {
    "iteration": 2,
    "total_timesteps": 1280,
    "total_episodes": 80,
    "mean_episode_reward": 10.8,
    "mean_episode_length": 16.0,
    "policy_lr": 0.008,
    "value_lr": 0.008,
    "policy_loss": -0.04570861533284187,
    "value_loss": 73.89225323994954,
    "mean_advantage": 5.9604645663569045e-09,
    "mean_return": 5.594352722167969
  },
  {
    "iteration": 3,
    "total_timesteps": 1920,
    "total_episodes": 120,
    "mean_episode_reward": 12.06,
    "mean_episode_length": 16.0,
    "policy_lr": 0.007,
    "value_lr": 0.007,
    "policy_loss": -0.043868938305725656,
    "value_loss": 77.85419845581055,
    "mean_advantage": -5.364417887676609e-08,
    "mean_return": 8.026679992675781
  },
  {
    "iteration": 4,
    "total_timesteps": 2560,
    "total_episodes": 160,
    "mean_episode_reward": 14.22,
    "mean_episode_length": 16.0,
    "policy_lr": 0.006000000000000001,
    "value_lr": 0.006000000000000001,
    "policy_loss": -0.04590899978453914,
    "value_loss": 61.33760070800781,
    "mean_advantage": -2.9802322387695312e-08,
    "mean_return": 7.540214538574219
  },
  {
    "iteration": 5,
    "total_timesteps": 3200,
    "total_episodes": 200,
    "mean_episode_reward": 15.3,
    "mean_episode_length": 16.0,
    "policy_lr": 0.005000000000000001,
    "value_lr": 0.005000000000000001,
    "policy_loss": -0.04821483309691151,
    "value_loss": 42.8602991104126,
    "mean_advantage": -1.1920929132713809e-08,
    "mean_return": 9.486077308654785
  },
  {
    "iteration": 6,
    "total_timesteps": 3840,
    "total_episodes": 240,
    "mean_episode_reward": 16.38,
    "mean_episode_length": 16.0,
    "policy_lr": 0.004000000000000001,
    "value_lr": 0.004000000000000001,
    "policy_loss": -0.03681884131704768,
    "value_loss": 31.80308485031128,
    "mean_advantage": 1.1920929132713809e-08,
    "mean_return": 9.242843627929688
  },
  {
    "iteration": 7,
    "total_timesteps": 4480,
    "total_episodes": 280,
    "mean_episode_reward": 17.28,
    "mean_episode_length": 16.0,
    "policy_lr": 0.003000000000000001,
    "value_lr": 0.003000000000000001,
    "policy_loss": -0.04621844785287976,
    "value_loss": 24.63386821746826,
    "mean_advantage": -2.9802322831784522e-09,
    "mean_return": 9.486077308654785
  },
  {
    "iteration": 8,
    "total_timesteps": 5120,
    "total_episodes": 320,
    "mean_episode_reward": 17.64,
    "mean_episode_length": 16.0,
    "policy_lr": 0.002000000000000001,
    "value_lr": 0.002000000000000001,
    "policy_loss": -0.04178335781519612,
    "value_loss": 21.48377784093221,
    "mean_advantage": 2.3841858265427618e-08,
    "mean_return": 9.729310035705566
  },
  {
    "iteration": 9,
    "total_timesteps": 5760,
    "total_episodes": 360,
    "mean_episode_reward": 17.82,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0010000000000000005,
    "value_lr": 0.0010000000000000005,
    "policy_loss": -0.030377482529729605,
    "value_loss": 20.672512531280518,
    "mean_advantage": -5.9604645663569045e-09,
    "mean_return": 9.729310035705566
  },
  {
    "iteration": 10,
    "total_timesteps": 6400,
    "total_episodes": 400,
    "mean_episode_reward": 18.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0,
    "value_lr": 0.0,
    "policy_loss": -0.017623984565337498,
    "value_loss": 20.816361745198567,
    "mean_advantage": 0.0,
    "mean_return": 9.729310035705566
  }
]