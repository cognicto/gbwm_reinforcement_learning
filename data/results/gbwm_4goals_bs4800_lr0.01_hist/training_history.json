[
  {
    "iteration": 1,
    "total_timesteps": 76800,
    "total_episodes": 4800,
    "mean_episode_reward": 30.82,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.09608227676091095,
    "value_loss": 156.65020516077678,
    "mean_advantage": 9.139378676081833e-08,
    "mean_return": 16.503034591674805
  },
  {
    "iteration": 2,
    "total_timesteps": 153600,
    "total_episodes": 9600,
    "mean_episode_reward": 41.22,
    "mean_episode_length": 16.0,
    "policy_lr": 0.008,
    "value_lr": 0.008,
    "policy_loss": -0.06982183892435084,
    "value_loss": 116.14299574534098,
    "mean_advantage": 9.53674295089968e-09,
    "mean_return": 23.75786018371582
  },
  {
    "iteration": 3,
    "total_timesteps": 230400,
    "total_episodes": 14400,
    "mean_episode_reward": 48.1,
    "mean_episode_length": 16.0,
    "policy_lr": 0.007,
    "value_lr": 0.007,
    "policy_loss": -0.05087405123437445,
    "value_loss": 67.17361271540324,
    "mean_advantage": 2.0662943356342112e-08,
    "mean_return": 27.928421020507812
  },
  {
    "iteration": 4,
    "total_timesteps": 307200,
    "total_episodes": 19200,
    "mean_episode_reward": 51.02,
    "mean_episode_length": 16.0,
    "policy_lr": 0.006000000000000001,
    "value_lr": 0.006000000000000001,
    "policy_loss": -0.03978221242936949,
    "value_loss": 40.48016488711039,
    "mean_advantage": 1.1126200405442432e-08,
    "mean_return": 29.66736602783203
  },
  {
    "iteration": 5,
    "total_timesteps": 384000,
    "total_episodes": 24000,
    "mean_episode_reward": 53.78,
    "mean_episode_length": 16.0,
    "policy_lr": 0.005000000000000001,
    "value_lr": 0.005000000000000001,
    "policy_loss": -0.0334384782325166,
    "value_loss": 29.04299602349599,
    "mean_advantage": -1.5894572324981482e-09,
    "mean_return": 30.350801467895508
  },
  {
    "iteration": 6,
    "total_timesteps": 460800,
    "total_episodes": 28800,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.004000000000000001,
    "value_lr": 0.004000000000000001,
    "policy_loss": -0.03094236318875725,
    "value_loss": 18.065686508814494,
    "mean_advantage": 6.357828929992593e-09,
    "mean_return": 30.763221740722656
  },
  {
    "iteration": 7,
    "total_timesteps": 537600,
    "total_episodes": 33600,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.003000000000000001,
    "value_lr": 0.003000000000000001,
    "policy_loss": -0.03074393986646707,
    "value_loss": 12.856091803709665,
    "mean_advantage": 1.5894572769070692e-08,
    "mean_return": 30.789634704589844
  },
  {
    "iteration": 8,
    "total_timesteps": 614400,
    "total_episodes": 38400,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.002000000000000001,
    "value_lr": 0.002000000000000001,
    "policy_loss": -0.028872886574827134,
    "value_loss": 10.956923160950343,
    "mean_advantage": -1.1126200405442432e-08,
    "mean_return": 30.809814453125
  },
  {
    "iteration": 9,
    "total_timesteps": 691200,
    "total_episodes": 43200,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0010000000000000005,
    "value_lr": 0.0010000000000000005,
    "policy_loss": -0.029128769758002212,
    "value_loss": 9.302795264720917,
    "mean_advantage": -1.2715657859985185e-08,
    "mean_return": 30.857648849487305
  },
  {
    "iteration": 10,
    "total_timesteps": 768000,
    "total_episodes": 48000,
    "mean_episode_reward": 54.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0,
    "value_lr": 0.0,
    "policy_loss": -0.0285301441179278,
    "value_loss": 8.842182938655217,
    "mean_advantage": -3.1789144649962964e-09,
    "mean_return": 30.85854148864746
  }
]