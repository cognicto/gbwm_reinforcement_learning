[
  {
    "iteration": 1,
    "total_timesteps": 76800,
    "total_episodes": 4800,
    "mean_episode_reward": 71.34,
    "mean_episode_length": 16.0,
    "policy_lr": 0.009000000000000001,
    "value_lr": 0.009000000000000001,
    "policy_loss": -0.12164570530566077,
    "value_loss": 417.76358169555664,
    "mean_advantage": -4.609425729995564e-08,
    "mean_return": 40.60723876953125
  },
  {
    "iteration": 2,
    "total_timesteps": 153600,
    "total_episodes": 9600,
    "mean_episode_reward": 96.8,
    "mean_episode_length": 16.0,
    "policy_lr": 0.008,
    "value_lr": 0.008,
    "policy_loss": -0.09249474911872918,
    "value_loss": 269.20801582336424,
    "mean_advantage": 0.0,
    "mean_return": 56.65650939941406
  },
  {
    "iteration": 3,
    "total_timesteps": 230400,
    "total_episodes": 14400,
    "mean_episode_reward": 115.02,
    "mean_episode_length": 16.0,
    "policy_lr": 0.007,
    "value_lr": 0.007,
    "policy_loss": -0.059822048568942895,
    "value_loss": 138.9329834874471,
    "mean_advantage": -6.993611378902642e-08,
    "mean_return": 67.3224105834961
  },
  {
    "iteration": 4,
    "total_timesteps": 307200,
    "total_episodes": 19200,
    "mean_episode_reward": 123.48,
    "mean_episode_length": 16.0,
    "policy_lr": 0.006000000000000001,
    "value_lr": 0.006000000000000001,
    "policy_loss": -0.04319536550203338,
    "value_loss": 69.32814815839131,
    "mean_advantage": 6.357829107628277e-08,
    "mean_return": 71.49667358398438
  },
  {
    "iteration": 5,
    "total_timesteps": 384000,
    "total_episodes": 24000,
    "mean_episode_reward": 124.66,
    "mean_episode_length": 16.0,
    "policy_lr": 0.005000000000000001,
    "value_lr": 0.005000000000000001,
    "policy_loss": -0.033805188174980384,
    "value_loss": 36.994693409601844,
    "mean_advantage": 6.357828929992593e-09,
    "mean_return": 73.16158294677734
  },
  {
    "iteration": 6,
    "total_timesteps": 460800,
    "total_episodes": 28800,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.004000000000000001,
    "value_lr": 0.004000000000000001,
    "policy_loss": -0.028522109047820172,
    "value_loss": 21.408713669776915,
    "mean_advantage": -3.1789145538141383e-08,
    "mean_return": 73.90564727783203
  },
  {
    "iteration": 7,
    "total_timesteps": 537600,
    "total_episodes": 33600,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.003000000000000001,
    "value_lr": 0.003000000000000001,
    "policy_loss": -0.027973741366683194,
    "value_loss": 20.731919719378155,
    "mean_advantage": 3.814697180359872e-08,
    "mean_return": 73.93563842773438
  },
  {
    "iteration": 8,
    "total_timesteps": 614400,
    "total_episodes": 38400,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.002000000000000001,
    "value_lr": 0.002000000000000001,
    "policy_loss": -0.028799079235953588,
    "value_loss": 19.805074100494384,
    "mean_advantage": 3.1789145538141383e-08,
    "mean_return": 73.94479370117188
  },
  {
    "iteration": 9,
    "total_timesteps": 691200,
    "total_episodes": 43200,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0010000000000000005,
    "value_lr": 0.0010000000000000005,
    "policy_loss": -0.02774883672206973,
    "value_loss": 18.14018464565277,
    "mean_advantage": 2.543131571997037e-08,
    "mean_return": 73.95354461669922
  },
  {
    "iteration": 10,
    "total_timesteps": 768000,
    "total_episodes": 48000,
    "mean_episode_reward": 126.0,
    "mean_episode_length": 16.0,
    "policy_lr": 0.0,
    "value_lr": 0.0,
    "policy_loss": -0.02857182515008996,
    "value_loss": 17.75240004936854,
    "mean_advantage": -6.357828929992593e-09,
    "mean_return": 73.95354461669922
  }
]